{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special as sp\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "\n",
    "# Necessary imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Linear\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.utils import algorithm_globals\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduce experience replay.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = collections.namedtuple('Experience',\n",
    "                                    field_names=['state', 'action',\n",
    "                                                 'next_state', 'reward',\n",
    "                                                 'is_game_on'])\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size, device = 'cuda'):\n",
    "        indices = np.random.choice(len(self.memory), batch_size, replace = False)\n",
    "        \n",
    "        states, actions, next_states, rewards, isgameon = zip(*[self.memory[idx] \n",
    "                                                                for idx in indices])\n",
    "        \n",
    "        return torch.Tensor(states).type(torch.float).to(device), \\\n",
    "               torch.Tensor(actions).type(torch.long).to(device), \\\n",
    "               torch.Tensor(next_states).to(device), \\\n",
    "               torch.Tensor(rewards).to(device), torch.tensor(isgameon).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Networks definition.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = 2\n",
    "\n",
    "def create_qnn():\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    feature_map = ZFeatureMap(num_qubits)\n",
    "    ansatz = RealAmplitudes(num_qubits=num_qubits, reps=1)\n",
    "\n",
    "    qc.compose(feature_map, inplace=True)\n",
    "    qc.compose(ansatz, inplace=True)\n",
    "\n",
    "    # REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
    "    qnn = SamplerQNN(\n",
    "        circuit=qc,\n",
    "        input_params=feature_map.parameters,\n",
    "        weight_params=ansatz.parameters,\n",
    "        input_gradients=True,\n",
    "    )\n",
    "    \n",
    "    return qnn\n",
    "\n",
    "qnn = create_qnn()\n",
    "print(\"QNN Training Parameters: \", qnn.num_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_nn(nn.Module):\n",
    "    \n",
    "    channels = [16, 32, 64]\n",
    "    kernels = [3, 3, 3]\n",
    "    strides = [1, 1, 1]\n",
    "    in_channels = 1\n",
    "    \n",
    "    def __init__(self, rows, cols, n_act):\n",
    "        super().__init__()\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels = self.in_channels,\n",
    "                                            out_channels = self.channels[0],\n",
    "                                            kernel_size = self.kernels[0],\n",
    "                                            stride = self.strides[0]),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Conv2d(in_channels = self.channels[0],\n",
    "                                            out_channels = self.channels[1],\n",
    "                                            kernel_size = self.kernels[1],\n",
    "                                            stride = self.strides[1]),\n",
    "                                  nn.ReLU()\n",
    "                                 )\n",
    "        \n",
    "        size_out_conv = self.get_conv_size(rows, cols)\n",
    "        self.linear = nn.Sequential(nn.Linear(size_out_conv, 2),\n",
    "                                    TorchConnector(qnn),\n",
    "                                    nn.Linear(4, n_act),\n",
    "                                   )\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x = x.view(len(x), self.in_channels, self.rows, self.cols)\n",
    "        out_conv = self.conv(x).view(len(x),-1)\n",
    "        out_lin = self.linear(out_conv)\n",
    "        return out_lin\n",
    "    \n",
    "    def get_conv_size(self, x, y):\n",
    "        out_conv = self.conv(torch.zeros(1,self.in_channels, x, y))\n",
    "        return int(np.prod(out_conv.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qloss(batch, net, gamma=0.99, device=\"cuda\"):\n",
    "    states, actions, next_states, rewards, _ = batch\n",
    "\n",
    "    # Move tensors to the desired device\n",
    "    states = states.to(device)\n",
    "    actions = actions.to(device)\n",
    "    next_states = next_states.to(device)\n",
    "    rewards = rewards.to(device)\n",
    "\n",
    "    # Convert states and next_states to NumPy arrays\n",
    "    states = states.cpu().detach().numpy()\n",
    "    next_states = next_states.cpu().detach().numpy()\n",
    "\n",
    "    # Create PyTorch tensors from the NumPy arrays\n",
    "    states_tensor = torch.from_numpy(states).to(device)\n",
    "    next_states_tensor = torch.from_numpy(next_states).to(device)\n",
    "\n",
    "    # Perform the rest of the computation on the device\n",
    "    lbatch = len(states_tensor)\n",
    "    state_action_values = net(states_tensor.view(lbatch,-1))\n",
    "    state_action_values = state_action_values.gather(1, actions.unsqueeze(-1))\n",
    "    state_action_values = state_action_values.squeeze(-1)\n",
    "    #state_action_values = torch.tensor(input_list).squeeze()\n",
    "    \n",
    "    next_state_values = net(next_states_tensor.view(lbatch, -1))\n",
    "    next_state_values = next_state_values.max(1)[0]\n",
    "    \n",
    "    next_state_values = next_state_values.detach()\n",
    "    expected_state_action_values = next_state_values * gamma + rewards\n",
    "    \n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the maze and define the environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import MazeEnvironment\n",
    "\n",
    "maze = np.load('./maze_generator/maze_10x10.npy')\n",
    "\n",
    "initial_position = [0,0]\n",
    "goal = [len(maze)-1, len(maze)-1]\n",
    "\n",
    "maze_env = MazeEnvironment(maze, initial_position, goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_env.draw('./results/maze_10.pdf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the agent and the buffer for experience replay.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_capacity = 10000\n",
    "buffer_start_size = 1000\n",
    "memory_buffer = ExperienceReplay(buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "agent = Agent(maze = maze_env,\n",
    "              memory_buffer = memory_buffer,\n",
    "              use_softmax = True\n",
    "             )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Define the network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = conv_nn(len(maze), len(maze), 4).to('cuda')\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 24\n",
    "gamma = 0.9\n",
    "\n",
    "net.to(device)\n",
    "print(\"Number of Trainable Parameters: \", sum(p.numel() for p in net.parameters() if p.requires_grad))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the epsilon profile and plot the resetting probability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 20000\n",
    "\n",
    "cutoff = 3000\n",
    "epsilon = np.exp(-np.arange(num_epochs)/(cutoff))\n",
    "epsilon[epsilon > epsilon[100*int(num_epochs/cutoff)]] = epsilon[100*int(num_epochs/cutoff)]\n",
    "plt.plot(epsilon, color = 'orangered', ls = '--')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.savefig('epsilon_profile.pdf', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "mp = []\n",
    "mpm = []\n",
    "reg = 200\n",
    "for e in epsilon:\n",
    "    a = agent.env.reset_policy(e)\n",
    "    mp.append(np.min(a))\n",
    "    mpm.append(np.max(a))\n",
    "\n",
    "plt.plot(epsilon/1.3, color = 'orangered', ls = '--', alpha = 0.5,\n",
    "         label= 'Epsilon profile (arbitrary units)')\n",
    "\n",
    "plt.plot(np.array(mpm)-np.array(mp), label = 'Probability difference', color = 'cornflowerblue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(r'max $p^r$ - min $p^r$')\n",
    "plt.legend()\n",
    "plt.savefig('reset_policy.pdf', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_log = []\n",
    "best_loss = 1e5\n",
    "\n",
    "running_loss = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = 0\n",
    "    counter = 0\n",
    "    eps = epsilon[epoch]\n",
    "    \n",
    "    agent.isgameon = True\n",
    "    _ = agent.env.reset(eps)\n",
    "    \n",
    "    while agent.isgameon:\n",
    "        agent.make_a_move(net, eps)\n",
    "        counter += 1\n",
    "        \n",
    "        if len(agent.buffer) < buffer_start_size:\n",
    "            continue\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        batch = agent.buffer.sample(batch_size, device = device)\n",
    "        loss_t = Qloss(batch, net, gamma = gamma, device = device)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss += loss_t.item()\n",
    "    \n",
    "    if (agent.env.current_position == agent.env.goal).all():\n",
    "        result = 'won'\n",
    "    else:\n",
    "        result = 'lost'\n",
    "    \n",
    "    if epoch%1000 == 0:\n",
    "        agent.plot_policy_map(net, 'sol_epoch_'+str(epoch)+'.pdf', [0.35,-0.3])\n",
    "    \n",
    "    loss_log.append(loss)\n",
    "    \n",
    "    if (epoch > 2000):\n",
    "        running_loss = np.mean(loss_log[-50:])\n",
    "        if running_loss < best_loss:\n",
    "            best_loss = running_loss\n",
    "            torch.save(net.state_dict(), \"best.torch\")\n",
    "            estop = epoch\n",
    "    \n",
    "    print('Epoch', epoch, '(number of moves ' + str(counter) + ')')\n",
    "    print('Game', result)\n",
    "    print('[' + '#'*(100-int(100*(1 - epoch/num_epochs))) +\n",
    "          ' '*int(100*(1 - epoch/num_epochs)) + ']')\n",
    "    print('\\t Average loss: ' + f'{loss:.5f}')\n",
    "    if (epoch > 2000):\n",
    "        print('\\t Best average loss of the last 50 epochs: ' + f'{best_loss:.5f}' + ', achieved at epoch', estop)\n",
    "    clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"./results/net.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epsilon*90, alpha = 0.6, ls = '--', label = 'Epsilon profile (arbitrary unit)', color = 'orangered')\n",
    "plt.plot((np.array(mpm)-np.array(mp))*120, alpha = 0.6, ls = '--',\n",
    "         label = 'Probability difference (arbitrary unit)', color = 'dimgray')\n",
    "plt.plot(loss_log, label = 'Loss', color = 'cornflowerblue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.savefig('loss.pdf', dpi = 300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the maze solution and the policy learnt.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "agent.isgameon = True\n",
    "agent.use_softmax = False\n",
    "_ = agent.env.reset(0)\n",
    "while agent.isgameon:\n",
    "    agent.make_a_move(net, 0)\n",
    "    agent.env.draw('')\n",
    "    clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_policy_map(net, './results/solution.pdf', [0.35,-0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net = copy.deepcopy(net)\n",
    "best_net.load_state_dict(torch.load('./results/best_classical.torch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_policy_map(best_net, './results/classical_solution_best.pdf', [0.35,-0.3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
