{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from IPython.display import  clear_output\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduce experience replay.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = collections.namedtuple('Experience',\n",
    "                                    field_names=['state', 'action',\n",
    "                                                 'next_state', 'reward',\n",
    "                                                 'is_game_on'])\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size, device='cuda'):\n",
    "        indices = np.random.choice(len(self.memory), batch_size, replace=False)\n",
    "        transitions = [self.memory[idx] for idx in indices]\n",
    "        states = np.stack([np.array(t[0]) for t in transitions])\n",
    "        actions = np.stack([np.array(t[1]) for t in transitions])\n",
    "        next_states = np.stack([np.array(t[2]) for t in transitions])\n",
    "        rewards = np.stack([np.array(t[3]) for t in transitions])\n",
    "        isgameon = np.stack([np.array(t[4]) for t in transitions])\n",
    "\n",
    "        return torch.from_numpy(states).to(device=device, dtype=torch.float), \\\n",
    "               torch.from_numpy(actions).to(device=device, dtype=torch.long), \\\n",
    "               torch.from_numpy(next_states).to(device=device, dtype=torch.float), \\\n",
    "               torch.from_numpy(rewards).to(device=device, dtype=torch.float), \\\n",
    "               torch.from_numpy(isgameon).to(device=device, dtype=torch.float)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Networks definition.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Linear\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.utils import algorithm_globals\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "# Set seed for random generators\n",
    "algorithm_globals.random_seed = 42\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parmas = [Parameter('theta_0'), Parameter('theta_1'), Parameter('theta_2'), Parameter('theta_3'), Parameter('theta_4'), Parameter('theta_5'), Parameter('theta_6'), Parameter('theta_7')]\n",
    "num_qubits = 8\n",
    "qc = QuantumCircuit(num_qubits)\n",
    "feature_map = ZZFeatureMap(num_qubits)\n",
    "qc.compose(feature_map, inplace=True)\n",
    "qc.ry(parmas[0], 0)\n",
    "qc.ry(parmas[1], 1)\n",
    "qc.ry(parmas[2], 2)\n",
    "qc.ry(parmas[3], 3)\n",
    "qc.ry(parmas[4], 4)\n",
    "qc.ry(parmas[5], 5)\n",
    "qc.ry(parmas[6], 6)\n",
    "qc.ry(parmas[7], 7)\n",
    "\n",
    "qc.draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
    "qnn = EstimatorQNN(\n",
    "    circuit=qc,\n",
    "    input_params=feature_map.parameters,\n",
    "    weight_params=parmas,\n",
    "    input_gradients=True,\n",
    ")\n",
    "\n",
    "qnn.num_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_nn(nn.Module):\n",
    "    def __init__(self, Ni, No = 4, qnn = qnn):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(Ni, Ni)\n",
    "        self.fc2 = nn.Linear(Ni, num_qubits)\n",
    "        self.qnn = TorchConnector(qnn)\n",
    "        self.fc3 = nn.Linear(1, 1)\n",
    "        self.fc5 = Linear(1, No) \n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.qnn(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qloss(batch, net, gamma=0.99, device=\"cuda\"):\n",
    "    states, actions, next_states, rewards, _ = batch\n",
    "\n",
    "    # Move tensors to the desired device\n",
    "    states = states.to(device)\n",
    "    actions = actions.to(device)\n",
    "    next_states = next_states.to(device)\n",
    "    rewards = rewards.to(device)\n",
    "\n",
    "    # Convert states and next_states to NumPy arrays\n",
    "    states = states.cpu().detach().numpy()\n",
    "    next_states = next_states.cpu().detach().numpy()\n",
    "\n",
    "    # Create PyTorch tensors from the NumPy arrays\n",
    "    states_tensor = torch.from_numpy(states).to(device)\n",
    "    next_states_tensor = torch.from_numpy(next_states).to(device)\n",
    "\n",
    "    # Perform the rest of the computation on the device\n",
    "    lbatch = len(states_tensor)\n",
    "    state_action_values = net(states_tensor.view(lbatch,-1))\n",
    "    state_action_values = state_action_values.gather(1, actions.unsqueeze(-1))\n",
    "    state_action_values = state_action_values.squeeze(-1)\n",
    "    #state_action_values = torch.tensor(input_list).squeeze()\n",
    "    \n",
    "    next_state_values = net(next_states_tensor.view(lbatch, -1))\n",
    "    next_state_values = next_state_values.max(1)[0]\n",
    "    \n",
    "    next_state_values = next_state_values.detach()\n",
    "    expected_state_action_values = next_state_values * gamma + rewards\n",
    "    \n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the maze and define the environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import MazeEnvironment\n",
    "\n",
    "maze = np.load('maze_10x10.npy')\n",
    "\n",
    "initial_position = [0,0]\n",
    "goal = [len(maze)-1, len(maze)-1]\n",
    "\n",
    "maze_env = MazeEnvironment(maze, initial_position, goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_env.draw('maze_10.pdf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the agent and the buffer for experience replay.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_capacity = 10000\n",
    "buffer_start_size = 1000\n",
    "memory_buffer = ExperienceReplay(buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "\n",
    "agent = Agent(maze = maze_env,\n",
    "              memory_buffer = memory_buffer,\n",
    "              use_softmax = True\n",
    "             )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Define the network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = fc_nn(maze.size, 4, qnn)\n",
    "print(\"Maze Size: \", maze.size/10)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "batch_size = 64\n",
    "print(\"Batch Size: \", batch_size)\n",
    "gamma = 0.95\n",
    "\n",
    "net.to(device)\n",
    "print(\"Number of trainable parameters: \", sum(p.numel() for p in net.parameters() if p.requires_grad))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the epsilon profile and plot the resetting probability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "cutoff = 600\n",
    "epsilon = np.exp(-np.arange(num_epochs)/(cutoff))\n",
    "epsilon[epsilon > epsilon[150*int(num_epochs/cutoff)]] = epsilon[150*int(num_epochs/cutoff)]\n",
    "plt.plot(epsilon, color = 'orangered', ls = '--')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.savefig('epsilon_profile.pdf', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "mp = []\n",
    "mpm = []\n",
    "reg = 200\n",
    "for e in epsilon:\n",
    "    a = agent.env.reset_policy(e)\n",
    "    mp.append(np.min(a))\n",
    "    mpm.append(np.max(a))\n",
    "\n",
    "plt.plot(epsilon/1.3, color = 'orangered', ls = '--', alpha = 0.5,\n",
    "         label= 'Epsilon profile (arbitrary units)')\n",
    "\n",
    "plt.plot(np.array(mpm)-np.array(mp), label = 'Probability difference', color = 'cornflowerblue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(r'max $p^r$ - min $p^r$')\n",
    "plt.legend()\n",
    "plt.savefig('reset_policy.pdf', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_loss = 1e5\n",
    "\n",
    "running_loss = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = 0\n",
    "    counter = 0\n",
    "    eps = epsilon[epoch]\n",
    "    \n",
    "    agent.isgameon = True\n",
    "    _ = agent.env.reset(eps)\n",
    "    \n",
    "    while agent.isgameon:\n",
    "        agent.make_a_move(net, eps)\n",
    "        counter += 1\n",
    "        \n",
    "        if len(agent.buffer) < buffer_start_size:\n",
    "            continue\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        batch = agent.buffer.sample(batch_size, device = device)\n",
    "        loss_t = Qloss(batch, net, gamma = gamma, device = device)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss += loss_t.item()\n",
    "    \n",
    "    if (agent.env.current_position == agent.env.goal).all():\n",
    "        result = 'won'\n",
    "    else:\n",
    "        result = 'lost'\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        agent.plot_policy_map(net, 'sol_epoch_'+str(epoch)+'.pdf', [0.35,-0.3])\n",
    "\n",
    "    \n",
    "    print('Epoch', epoch, '(number of moves ' + str(counter) + ')')\n",
    "    print('Game', result)\n",
    "    print('[' + '#'*(100-int(100*(1 - epoch/num_epochs))) +\n",
    "          ' '*int(100*(1 - epoch/num_epochs)) + ']')\n",
    "    print('\\t Average loss: ' + f'{loss:.5f}')\n",
    "    \n",
    "    if (epoch > 2000):\n",
    "        print('\\t Best average loss of the last 50 epochs: ' + f'{best_loss:.5f}' + ', achieved at epoch', estop)\n",
    "    clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"quantumDNN.torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epsilon*90, alpha = 0.6, ls = '--', label = 'Epsilon profile (arbitrary unit)', color = 'orangered')\n",
    "plt.plot((np.array(mpm)-np.array(mp))*120, alpha = 0.6, ls = '--',\n",
    "         label = 'Probability difference (arbitrary unit)', color = 'dimgray')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.savefig('loss.pdf', dpi = 300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the maze solution and the policy learnt.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "agent.isgameon = True\n",
    "agent.use_softmax = False\n",
    "_ = agent.env.reset(0)\n",
    "while agent.isgameon:\n",
    "    agent.make_a_move(net, 0)\n",
    "    agent.env.draw('')\n",
    "    clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_policy_map(net, 'solution.pdf', [0.35,-0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net = copy.deepcopy(net)\n",
    "best_net.load_state_dict(torch.load('best.torch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_policy_map(best_net, 'solution_best.pdf', [0.35,-0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Version Information</h3><table><tr><th>Qiskit Software</th><th>Version</th></tr><tr><td><code>qiskit-terra</code></td><td>0.23.2</td></tr><tr><td><code>qiskit-aer</code></td><td>0.11.2</td></tr><tr><td><code>qiskit-ignis</code></td><td>0.6.0</td></tr><tr><td><code>qiskit-ibmq-provider</code></td><td>0.20.1</td></tr><tr><td><code>qiskit</code></td><td>0.41.1</td></tr><tr><td><code>qiskit-finance</code></td><td>0.3.4</td></tr><tr><td><code>qiskit-optimization</code></td><td>0.4.0</td></tr><tr><td><code>qiskit-machine-learning</code></td><td>0.7.0</td></tr><tr><th>System information</th></tr><tr><td>Python version</td><td>3.8.10</td></tr><tr><td>Python compiler</td><td>GCC 9.4.0</td></tr><tr><td>Python build</td><td>default, Jun 22 2022 20:18:18</td></tr><tr><td>OS</td><td>Linux</td></tr><tr><td>CPUs</td><td>12</td></tr><tr><td>Memory (Gb)</td><td>15.435047149658203</td></tr><tr><td colspan='2'>Thu May 04 20:57:27 2023 CST</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='width: 100%; background-color:#d5d9e0;padding-left: 10px; padding-bottom: 10px; padding-right: 10px; padding-top: 5px'><h3>This code is a part of Qiskit</h3><p>&copy; Copyright IBM 2017, 2023.</p><p>This code is licensed under the Apache License, Version 2.0. You may<br>obtain a copy of this license in the LICENSE.txt file in the root directory<br> of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.<p>Any modifications or derivative works of this code must retain this<br>copyright notice, and modified files need to carry a notice indicating<br>that they have been altered from the originals.</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import qiskit.tools.jupyter\n",
    "\n",
    "%qiskit_version_table\n",
    "%qiskit_copyright"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
